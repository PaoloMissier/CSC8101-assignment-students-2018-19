{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Betweenness\n",
    "\n",
    "Notebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "\n",
    "SPARK_HOME=\"/home/human/l/rh/platform/spark-2.3.0-bin-hadoop2.7\"\n",
    "DATA = \"data/\"\n",
    "OUTPUT = DATA + \"out/\"\n",
    "\n",
    "## SparkSession is one main connection with the Spark driver\n",
    "session = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Distributed-Community-Scanner\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", '-1') \\\n",
    "    .config(\"spark.ui.enabled\", 'true')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent project to the syspath to be able to import the modules\n",
    "# https://stackoverflow.com/questions/34478398/import-local-function-from-a-module-housed-in-another-directory-with-relative-im\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "# Import modules\n",
    "from inout.parser import parse\n",
    "from inout.parser import get_headers\n",
    "from view.visualiser import draw\n",
    "from algorithms.shortest_path import single_source_shortest_paths_dijkstra\n",
    "from distributed_algorithms.betweenness import compute_edge_betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edges_file = 'edges.csv'\n",
    "edges_limit = 100  # maximum numbers of edges to be parsed.\n",
    "edges_file = 'girvan_graph.csv'\n",
    "edges_path = os.path.join(DATA, edges_file)\n",
    "headers = get_headers(edges_path)\n",
    "source_header, target_header, weight_header = headers\n",
    "\n",
    "# Parse with edges limit:\n",
    "# graph = parse(edges_path, edge_limit=edges_limit, \n",
    "#               source_header=source_header, target_header=target_header, weight_header=weight_header)\n",
    "\n",
    "# Parse the entire graph (no 'edge_limit' parameter):\n",
    "graph = parse(edges_path,  # edge_limit=edges_limit, \n",
    "              source_header=source_header, target_header=target_header, weight_header=weight_header)\n",
    "\n",
    "len(list(graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Graph and Algorithms to Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dependencies to workers\n",
    "dependencies_path = \"community_scanner.zip\"\n",
    "sc.addPyFile(dependencies_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphBC = sc.broadcast(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sssp(source):\n",
    "    \"\"\"\n",
    "    Single-Source Shortest path function for the Spark workers.\n",
    "    Prerrequisite: the 'algorithms' module needs to be added as dependency to all workers with sc.addPyFile  \n",
    "    :param source: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    from algorithms.shortest_path import single_source_shortest_paths_dijkstra\n",
    "    paths, distances = single_source_shortest_paths_dijkstra(graphBC.value, source=source)\n",
    "    return (source, paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelise node IDs into RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6, 8, 2, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_rdd = sc.parallelize(graph.nodes) \n",
    "nodes_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 4, 2, 2, 4, 2, 4]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of elements in each partition\n",
    "nodes_rdd.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute shortests paths in workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  {1: [[1]],\n",
       "   2: [[1, 6, 2], [1, 8, 2]],\n",
       "   3: [[1, 6, 2, 3], [1, 8, 2, 3], [1, 6, 4, 3]],\n",
       "   4: [[1, 6, 4]],\n",
       "   5: [[1, 6, 2, 5], [1, 8, 2, 5], [1, 8, 7, 5]],\n",
       "   6: [[1, 6]],\n",
       "   7: [[1, 8, 7]],\n",
       "   8: [[1, 8]],\n",
       "   9: [[1, 6, 12, 10, 9]],\n",
       "   10: [[1, 6, 12, 10]],\n",
       "   11: [[1, 6, 12, 15, 11]],\n",
       "   12: [[1, 6, 12]],\n",
       "   13: [[1, 6, 12, 15, 11, 13], [1, 6, 12, 15, 14, 13]],\n",
       "   14: [[1, 6, 12, 15, 14]],\n",
       "   15: [[1, 6, 12, 15]],\n",
       "   16: [[1, 6, 12, 15, 16]],\n",
       "   17: [[1, 6, 2, 3, 21, 17],\n",
       "    [1, 8, 2, 3, 21, 17],\n",
       "    [1, 6, 4, 3, 21, 17],\n",
       "    [1, 6, 2, 5, 22, 17],\n",
       "    [1, 8, 2, 5, 22, 17],\n",
       "    [1, 8, 7, 5, 22, 17],\n",
       "    [1, 6, 12, 10, 18, 17],\n",
       "    [1, 6, 12, 15, 16, 17]],\n",
       "   18: [[1, 6, 12, 10, 18]],\n",
       "   19: [[1, 6, 2, 3, 21, 19],\n",
       "    [1, 8, 2, 3, 21, 19],\n",
       "    [1, 6, 4, 3, 21, 19],\n",
       "    [1, 6, 12, 10, 18, 19]],\n",
       "   20: [[1, 6, 12, 10, 18, 20]],\n",
       "   21: [[1, 6, 2, 3, 21], [1, 8, 2, 3, 21], [1, 6, 4, 3, 21]],\n",
       "   22: [[1, 6, 2, 5, 22], [1, 8, 2, 5, 22], [1, 8, 7, 5, 22]]})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shortest_paths_rdd := [(source, paths)]\n",
    "# paths := {target: [shortest_path]}\n",
    "# shortest_path := [node]\n",
    "\n",
    "shortest_paths_rdd = nodes_rdd.map(sssp)\n",
    "shortest_paths_rdd.cache()\n",
    "shortest_paths_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Betweeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 1), ((1, 2), 2), ((1, 3), 3), ((1, 4), 1), ((1, 5), 3)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shortest_paths_rdd := [(source, paths_dict)]\n",
    "# paths_dict := {target: [shortest_path]}\n",
    "# shortest_path := [node]\n",
    "\n",
    "\n",
    "def map_paths_count(rdd_row):\n",
    "    source, paths_dict = rdd_row\n",
    "    \n",
    "    paths_count = list()\n",
    "    \n",
    "    for (target, paths_list) in paths_dict.items():\n",
    "        \n",
    "        u, v = min(source, target), max(source, target)\n",
    "        \n",
    "        paths_count.append( \n",
    "            ((u, v), len(paths_list))\n",
    "        )\n",
    "        \n",
    "    return paths_count\n",
    "\n",
    "\n",
    "# paths_count_rdd := [((source, target), paths_count)]\n",
    "paths_count_rdd = shortest_paths_rdd.flatMap(map_paths_count)\n",
    "paths_count_rdd.cache()\n",
    "paths_count_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 2), ((2, 8), 1.0)),\n",
       " ((1, 2), ((1, 8), 1.0)),\n",
       " ((1, 2), ((2, 6), 1.0)),\n",
       " ((1, 2), ((1, 6), 1.0)),\n",
       " ((1, 3), ((1, 8), 1.0))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_edge_count(paths_list):\n",
    "    \"\"\"\n",
    "\n",
    "    :param paths_list:\n",
    "    :return: key = edge, value = count of shortest paths through that edge to this target\n",
    "    \"\"\"\n",
    "    edge_count = dict()\n",
    "\n",
    "    for path in paths_list:\n",
    "        for i in range(len(path) - 1):\n",
    "            \n",
    "            u, v = min(path[i], path[i + 1]), max(path[i], path[i + 1])\n",
    "            \n",
    "            edge = (u, v)\n",
    "            edge_count[edge] = edge_count.get(edge, 0) + 1.0\n",
    "\n",
    "    return edge_count\n",
    "\n",
    "\n",
    "def map_edges_count(rdd_row):\n",
    "    source, paths_dict = rdd_row\n",
    "    \n",
    "    edges_count_list = list()\n",
    "    \n",
    "    for (target, paths_list) in paths_dict.items():\n",
    "        for (edge, edge_count) in get_edge_count(paths_list).items():\n",
    "            \n",
    "            u, v = min(source, target), max(source, target)\n",
    "            \n",
    "            edges_count_list.append((\n",
    "                (u, v), (edge, edge_count)\n",
    "            ))\n",
    "     \n",
    "    return edges_count_list\n",
    "\n",
    "\n",
    "# edges_count_rdd := [((source, target), (edge, edge_count))]\n",
    "# (source, target): define the nodes for shortest path computation, i.e. this is not an edge\n",
    "# edge := (u, v)\n",
    "# edge_count: number of shortest paths between 'source' and 'target' that goes through 'edge'\n",
    "edges_count_rdd = shortest_paths_rdd.flatMap(map_edges_count)\n",
    "edges_count_rdd.cache()\n",
    "edges_count_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((2, 8), 69.9047619047619),\n",
       " ((6, 12), 191.4793650793651),\n",
       " ((11, 15), 106.36190476190477),\n",
       " ((5, 22), 135.8968253968254),\n",
       " ((13, 14), 45.63809523809524)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [((source, target), ((edge, edge_count), paths_count)]\n",
    "edges_path_count_rdd = edges_count_rdd.join(paths_count_rdd)\n",
    "\n",
    "# [(edge, betweenness)]\n",
    "# this produces the edge betweennes per path, \n",
    "# i.e. one edge can appear several times in this rdd,\n",
    "# one time per each shortest path\n",
    "edges_betweenness_per_path_rdd = edges_path_count_rdd.map(lambda row: (\n",
    "    row[1][0][0],  # edge \n",
    "    row[1][0][1] / row[1][1]  # edge_count / path_count  (sigma_st(e) / sigma_st in Girvan-Newman paper)  \n",
    "))\n",
    "\n",
    "# [(edge, betweenness)]\n",
    "# aggregates (sums) the betweenness of the edges\n",
    "# In this rdd each edge appears only once.\n",
    "\n",
    "edge_betweenness_rdd = edges_betweenness_per_path_rdd.reduceByKey(\n",
    "    lambda accumulator, edge_count: accumulator + edge_count\n",
    ")\n",
    "\n",
    "edge_betweenness_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert edge_betweenness_rdd.count() == len(list(graph.edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[44] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_betweenness_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_paths_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
